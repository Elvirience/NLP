{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "path = 'images/'\n",
        "label = os.listdir(path)\n",
        "\n",
        "img_names = os.listdir(path)\n",
        "docs_df = pd.DataFrame({'img_names':img_names})\n",
        "\n",
        "docs_df['text'] = ''\n",
        "docs_df.reset_index(drop=True, inplace=True)\n",
        "docs_df"
      ],
      "metadata": {
        "id": "bk3yavC6NNWq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# документы хранятся в json-файле в виде: имя файла фотографии: данные: {список слов текста документа}\n",
        "with open('original_merged_data.json') as data_file:\n",
        "    data_dict = json.load(data_file)\n",
        "data_dict\n",
        "\n",
        "for key in data_dict:\n",
        "    text = ' '.join(data_dict[key]['data'])\n",
        "    docs_df.loc[docs_df['img_names'] == key, ['text']] = text\n",
        "docs_df = docs_df.loc[docs_df['text'] != '']\n",
        "docs_df"
      ],
      "metadata": {
        "id": "bvNlQK8LNQTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('ner_labels.json', 'r') as labels_file:\n",
        "    labels_for_any_docs_dict = json.load(labels_file)\n",
        "labels_for_any_docs_dict"
      ],
      "metadata": {
        "id": "2tCKMieFNSGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "entity_types = []\n",
        "for i in labels_for_any_docs_dict:\n",
        "    for j in range(len(labels_for_any_docs_dict[i])):\n",
        "        entity_types.append(labels_for_any_docs_dict[i][j][0])\n",
        "entity_types = set(entity_types)\n",
        "print(f' Число сущностей: {len(entity_types)}\\n', entity_types)"
      ],
      "metadata": {
        "id": "HEm2f8tSNVkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Одна сущность может называться по-разному, напимер: \"KPP\" и \"кпп\", приведу сущности к единым обозначениям"
      ],
      "metadata": {
        "id": "8qiZH70fNcEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# для удобства просмотра и редактирования перенесу список сущностей в файл\n",
        "\n",
        "with open('Сущности.txt', 'w', encoding='utf-8') as f:\n",
        "    for i in entity_types:\n",
        "        f.write(i + '\\n')"
      ],
      "metadata": {
        "id": "Wz-eqH9wNd-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# после сопоставления в файле взаимозаменяемых сущностей вношу изменения в словарь\n",
        "\n",
        "translater_dict = {}\n",
        "\n",
        "with open('Сущности_ред.txt', 'r', encoding='utf-8') as f:\n",
        "    for i in f.readlines():\n",
        "        word2translate = i.split()\n",
        "        translater_dict[word2translate[0]]  = word2translate[1]\n",
        "\n",
        "for i in labels_for_any_docs_dict:\n",
        "    for j in range(len(labels_for_any_docs_dict[i])):\n",
        "        if labels_for_any_docs_dict[i][j][0] in translater_dict:\n",
        "            labels_for_any_docs_dict[i][j][0] = translater_dict[labels_for_any_docs_dict[i][j][0]]\n",
        "labels_for_any_docs_dict"
      ],
      "metadata": {
        "id": "5IQ-mmH2Nfxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Предобработка данных"
      ],
      "metadata": {
        "id": "lD13wL2LNkdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### В данных пристутсвуют выбросы. Поскольку данные были получены на основе разметки человека, предположим, что выбросы можно корректно обработать скриптом, не прибегая к повторной разметке"
      ],
      "metadata": {
        "id": "GmzGCQFoNlOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from string import punctuation\n",
        "import re\n",
        "\n",
        "\n",
        "def get_entry_left(string, sentence):\n",
        "    for i in range(len(sentence)):\n",
        "        if string != '' and string != ' ':\n",
        "            if string in sentence:\n",
        "                return string\n",
        "            else:\n",
        "                string = string[:-1]\n",
        "        else: return None\n",
        "\n",
        "\n",
        "def get_entry_right(string, sentence):\n",
        "    for i in range(len(sentence)):\n",
        "        if string != '' and string != ' ':\n",
        "            if string in sentence:\n",
        "                return string\n",
        "            else:\n",
        "                string = string[1:]\n",
        "        else: return None\n",
        "\n",
        "\n",
        "def get_max_word(_string, string_):\n",
        "    if _string != None and string_ != None:\n",
        "        r = len(_string)\n",
        "        l = len(string_)\n",
        "        if max(r, l) == r: return _string\n",
        "        else: return string_\n",
        "    elif _string != None and string_ == None:\n",
        "        return _string\n",
        "    elif _string == None and string_ != None:\n",
        "        return string_\n",
        "    else: return None\n",
        "\n",
        "\n",
        "def complex_estimation(s, string):\n",
        "    _s = get_entry_right(s, string)\n",
        "    s_ = get_entry_left(s, string)\n",
        "    return get_max_word(_s, s_)\n",
        "\n",
        "\n",
        "def isfloat(string):\n",
        "    try:\n",
        "        float(string)\n",
        "        return True\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# фильтр нерелевантных значений для каждой сущности\n",
        "# оценка некоторых пороговых значений для длины сущности основана на личном опыте работы с разметкой ...\n",
        "# ... имеющихся в данной работе данных\n",
        "pattern = '[%s]+' % re.escape(punctuation)\n",
        "def bad_word(entity_type, entity):\n",
        "    var = False\n",
        "    if entity_type == 'ADDRESSBUYER':\n",
        "        if len(entity) <= 21: var = True\n",
        "    if entity_type == 'ADDRESSSHIPPER':\n",
        "        if len(entity) <= 4: var = True\n",
        "    if entity_type == 'ADDRESSSELLER':\n",
        "        if len(entity) <= 16: var = True\n",
        "    if entity_type == 'AMOUNT':\n",
        "        if isfloat(entity) == False: var = True\n",
        "        elif re.match(r'[0]\\d+', entity) != None: var = True\n",
        "    if entity_type == 'BPINN':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "        elif len(entity) != 10: var = True\n",
        "    if entity_type == 'BPKPP':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "        elif len(entity) != 10: var = True\n",
        "    if entity_type == 'BPNAME':\n",
        "        if len(entity) < 9: var = True\n",
        "        elif len(entity) < 10 and entity in ['ОБЩЕСТВО С', 'Общество ', 'ОБЩЕСТВО ', 'Стоимость']: var = True\n",
        "    if entity_type == 'CONTENT':\n",
        "        if len(entity) < 3 or entity in ['ливо', ' 116']: var = True\n",
        "    if entity_type == 'CURRENCY':\n",
        "        if len(entity) < 8: var = True\n",
        "    if entity_type == 'CURRENCYKOD':\n",
        "        if entity.isalpha(): var = True\n",
        "    if entity_type == 'DOCDATE':\n",
        "        if len(entity) < 10: var = True\n",
        "        if ''.join(entity.split()).isalpha(): var = True\n",
        "    if entity_type == 'DOGDATE':\n",
        "        if ''.join(entity.split()).isalpha(): var = True\n",
        "        elif len(entity) < 4: var = True\n",
        "    if entity_type == 'DOGNUM':\n",
        "        if ''.join(entity.split()).isalpha(): var = True\n",
        "    if entity_type == 'INNBUYER':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "        elif len(entity) != 10: var = True\n",
        "    if entity_type == 'KPPBUYER':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "        elif len(entity) != 10: var = True\n",
        "    if entity_type == 'MEASUREUNIT':\n",
        "        if len(re.sub(pattern, '', ''.join(entity.split()))) == 1: var = True\n",
        "    if entity_type == 'NAMEBUYER':\n",
        "        if len(entity) < 11: var = True\n",
        "    if entity_type == 'PRICE':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "    if entity_type == 'SFNUM':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "        elif ''.join(entity.split()) in punctuation: var = True\n",
        "    if entity_type == 'SHIPPERNAME':\n",
        "        if len(entity) < 5: var = True\n",
        "        elif len(entity) < 11 and entity in ['Акционерно', 'Общество ', 'ОБЩЕСТВО ', 'Стоимость']: var = True\n",
        "    if entity_type == 'UNITPRICE':\n",
        "        if isfloat(entity) == False: var = True\n",
        "        elif re.match(r'[.]\\d+', entity) != None: var = True\n",
        "        elif re.match(r'[0]\\d+', entity) != None: var = True\n",
        "\n",
        "    # эксперимент\n",
        "    if entity_type == 'расчетный-счет-плательщика':\n",
        "        if len(entity) != 20: var = True\n",
        "    if entity_type == 'расчетный-счет-покупателя':\n",
        "        if len(entity) != 20: var = True\n",
        "    if entity_type == 'расчетный-счет-получателя':\n",
        "        if len(entity) != 20: var = True\n",
        "    if entity_type == 'кор-счет-банка-получателя':\n",
        "        if len(entity) != 20: var = True\n",
        "    if entity_type == 'кор-счет-банка-плательщика':\n",
        "        if len(entity) != 20: var = True\n",
        "    if entity_type == 'лицевой-счет-исполнителя':\n",
        "        if len(entity) != 20: var = True\n",
        "    if entity_type == 'бик-банка-плательщика':\n",
        "        if len(entity) != 9: var = True\n",
        "    if entity_type == 'бик-банка-получателя':\n",
        "        if len(entity) != 9: var = True\n",
        "    if entity_type == 'содержание-хоз-операции':\n",
        "        if len(entity) < 7 and entity != 'ГСМ': var = True\n",
        "    if entity_type == 'сумма-прописью':\n",
        "        if len(entity) < 13: var = True\n",
        "    if entity_type == 'ADDROBJ':\n",
        "        if len(entity) < 12: var = True\n",
        "    if entity_type == 'наименование-плательщика':\n",
        "        if len(entity) < 10: var = True\n",
        "    if entity_type == 'TOTALWITHNDS':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "    if entity_type == 'дата-акта':\n",
        "        if len(entity) < 10: var = True\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "    if entity_type == 'наименование-получателя':\n",
        "        if len(entity) < 10: var = True\n",
        "    if entity_type == 'SHIPPERBANK':\n",
        "        if len(entity) < 12: var = True\n",
        "    if entity_type == 'банк-плательщика':\n",
        "        if entity == ' БАНК ПАО': var = True\n",
        "    if entity_type == 'банк-получателя':\n",
        "        if len(entity) < 7: var = True\n",
        "    if entity_type == 'номер-акта':\n",
        "        if re.sub(pattern, '', ''.join(entity.split())).isalpha(): var = True\n",
        "    return var"
      ],
      "metadata": {
        "id": "1duJk-zNNnq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_labels_dict = {}\n",
        "for i in docs_df['img_names']:\n",
        "    text = docs_df['text'].loc[docs_df['img_names'] == i].values[0]\n",
        "    if i in labels_for_any_docs_dict:\n",
        "        entities_list = []\n",
        "        for j in labels_for_any_docs_dict[i]:\n",
        "            entity_type, entity = j\n",
        "            word_list = []\n",
        "            entity_text = ''.join(entity)\n",
        "            for word in entity:\n",
        "                word = complex_estimation(word, text)\n",
        "                if word != None and word not in punctuation:\n",
        "                    word_list.append(word)\n",
        "            if len(word_list) != 0 and ''.join(word_list) in text:\n",
        "                entity_text = ''.join(word_list)\n",
        "                if bad_word(entity_type, entity_text) == False:\n",
        "                    entity_text_length = len(entity_text)\n",
        "                    start_index = text.find(entity_text)\n",
        "                    entities_list.append({'entity_type': entity_type, 'entity_text': entity_text, 'start': start_index, 'end': start_index + entity_text_length})\n",
        "            elif entity_text in text:\n",
        "                entity_text = complex_estimation(entity_text, text)\n",
        "                if entity_text != None and entity_text not in punctuation and bad_word(entity_type, entity_text) == False:\n",
        "                    entity_text_length = len(entity_text)\n",
        "                    start_index = text.find(entity_text)\n",
        "                    entities_list.append({'entity_type': entity_type, 'entity_text': entity_text, 'start': start_index, 'end': start_index + entity_text_length})\n",
        "        ner_labels_dict[i] = {'text': text, 'entities': entities_list}"
      ],
      "metadata": {
        "id": "VWfraYRFNrHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_labels_dict"
      ],
      "metadata": {
        "id": "Xh1JsWuyNtXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lens_list = []\n",
        "for i in ner_labels_dict:\n",
        "    for j in ner_labels_dict[i]['entities']:\n",
        "        if j['entity_type'] == 'номер-акта':\n",
        "            lens_list.append(len(j['entity_text']))\n",
        "            if len(j['entity_text']) < 100:\n",
        "                print(j['entity_text'])"
      ],
      "metadata": {
        "id": "7iwf7U6LNu6I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### визуализация распределений длин некоторых сущностей"
      ],
      "metadata": {
        "id": "jqPEw3w6N0zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.xticks(range(0, 50, 2))\n",
        "sns.histplot(lens_list)"
      ],
      "metadata": {
        "id": "Ra00jLsdN2AM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Процесс токенизации"
      ],
      "metadata": {
        "id": "Nkpuue0TN5u1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from razdel import tokenize\n",
        "\n",
        "def extract_labels(item):\n",
        "    raw_toks = list(tokenize(item['text']))\n",
        "    words = [tok.text for tok in raw_toks]\n",
        "    word_labels = ['O'] * len(raw_toks)\n",
        "    char2word = [None] * len(item['text'])\n",
        "    for i, word in enumerate(raw_toks):\n",
        "        char2word[word.start:word.stop] = [i] * len(word.text)\n",
        "\n",
        "    for e in item['entities']:\n",
        "        e_words = sorted({idx for idx in char2word[e['start']:e['end']] if idx is not None})\n",
        "        word_labels[e_words[0]] = 'B-' + e['entity_type']\n",
        "        for idx in e_words[1:]:\n",
        "            word_labels[idx] = 'I-' + e['entity_type']\n",
        "\n",
        "    return {'tokens': words, 'tags': word_labels}"
      ],
      "metadata": {
        "id": "uFa_7IxyN74N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### проверка"
      ],
      "metadata": {
        "id": "pDpixjmVN98D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.choice(list(ner_labels_dict.keys()))"
      ],
      "metadata": {
        "id": "HDXGD_M_N-LU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = ner_labels_dict['10393498_1_page000.jpg']\n",
        "for i in range(len(extract_labels(e)['tokens'])):\n",
        "    if extract_labels(e)['tags'][i] != 'O':\n",
        "        print(extract_labels(e)['tokens'][i], extract_labels(e)['tags'][i])"
      ],
      "metadata": {
        "id": "GOO28MwWN_5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "разбиение данных на тренировочную и тестовую выборки"
      ],
      "metadata": {
        "id": "VD5wbT8YOE27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "ner_data = [extract_labels(item) for _, item in ner_labels_dict.items()]\n",
        "ner_train, ner_test = train_test_split(ner_data, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "C5mEhddiOGKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.options.display.max_colwidth = 300\n",
        "pd.DataFrame(ner_train).sample(3)"
      ],
      "metadata": {
        "id": "76mQwtCMOHzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_list = sorted({label for item in ner_data for label in item['tags']})\n",
        "if 'O' in label_list:\n",
        "    label_list.remove('O')\n",
        "    label_list = ['O'] + label_list\n",
        "label_list"
      ],
      "metadata": {
        "id": "Dh7MgMG_OJcN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Составим датасет"
      ],
      "metadata": {
        "id": "URA2CX8GOOJn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict"
      ],
      "metadata": {
        "id": "csDwv_KiOPKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ner_data = DatasetDict({\n",
        "    'train': Dataset.from_pandas(pd.DataFrame(ner_train)),\n",
        "    'test': Dataset.from_pandas(pd.DataFrame(ner_test))\n",
        "})\n",
        "ner_data"
      ],
      "metadata": {
        "id": "kfTh-7XuOSVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "checkpoint = \"rubert-tiny2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ],
      "metadata": {
        "id": "AQEA3lhhOULr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples, label_all_tokens=False):\n",
        "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\n",
        "    labels = []\n",
        "    for i, label in enumerate(examples['tags']):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        label_ids = [label_list.index(idx) if isinstance(idx, str) else idx for idx in label_ids]\n",
        "\n",
        "        labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = labels\n",
        "    return tokenized_inputs"
      ],
      "metadata": {
        "id": "XaaOddtfOV01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenize_and_align_labels(ner_data['train'][22:23])"
      ],
      "metadata": {
        "id": "INe77kCDOYHj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# токенизация датасета\n",
        "\n",
        "tokenized_datasets = ner_data.map(tokenize_and_align_labels, batched=True)\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "id": "3RmrM9h9OZmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning"
      ],
      "metadata": {
        "id": "bRUSu_EqOf2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(checkpoint, num_labels=len(label_list))\n",
        "model.config.id2label = dict(enumerate(label_list))\n",
        "model.config.label2id = {v: k for k, v in model.config.id2label.items()}"
      ],
      "metadata": {
        "id": "-QT6oY1jOhwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ],
      "metadata": {
        "id": "dwci_4wKOiDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "def compute_metrics(p):\n",
        "    predictions, labels, inputs = p.predictions, p.label_ids, p.inputs\n",
        "    predictions = np.argmax(p.predictions, axis=2)\n",
        "\n",
        "    # send only the first token of each word to the evaluation\n",
        "    true_predictions = []\n",
        "    true_labels = []\n",
        "    for prediction, label, tokens in zip(predictions, labels, inputs):\n",
        "        true_predictions.append([])\n",
        "        true_labels.append([])\n",
        "        for (p, l, t) in zip(prediction, label, tokens):\n",
        "            if l != -100 and not tokenizer.convert_ids_to_tokens(int(t)).startswith('##'):\n",
        "                true_predictions[-1].append(label_list[p])\n",
        "                true_labels[-1].append(label_list[l])\n",
        "\n",
        "    labels = [j for i in true_labels for j in i]\n",
        "    preds = [j for i in true_predictions for j in i]\n",
        "    return {'Accuracy': accuracy_score(labels, preds),\n",
        "            'F1': f1_score(labels, preds, average=\"weighted\")}"
      ],
      "metadata": {
        "id": "XGr_e5GUOj52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 5\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"ner\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=1e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    save_strategy='no',\n",
        "    report_to='none',\n",
        "    include_inputs_for_metrics=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "EJ36LQnZOmQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "QwNv4-EXOpxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "JnVgCQ9fOoOp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### проверка"
      ],
      "metadata": {
        "id": "TMhTC4b6Ot3x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "pipe = pipeline(model=model, tokenizer=tokenizer, task='ner', aggregation_strategy='average')"
      ],
      "metadata": {
        "id": "aRcMJXVKOwnw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# вспомогательная функция для удобного отображения предсказания\n",
        "def get_pred(idx=None):\n",
        "    if idx == None:\n",
        "        idx = random.choice(range(len(ner_test)))\n",
        "    text = ' '.join(ner_test[idx]['tokens'])\n",
        "    print(text)\n",
        "    for i in pipe(text):\n",
        "        print(i)"
      ],
      "metadata": {
        "id": "9OT6hhxcOxP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_pred()"
      ],
      "metadata": {
        "id": "lV3yO-lcOyu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(2):\n",
        "    get_pred()\n",
        "    print('\\n\\n')"
      ],
      "metadata": {
        "id": "nCDjqFQ1O1Ki"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}